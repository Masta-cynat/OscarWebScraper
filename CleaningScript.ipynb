{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Masta-cynat/OscarWebScraper/blob/main/CleaningScript.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIcHW9e3R_WM",
        "outputId": "5dc6cfda-4cff-4b07-f977-753a80fd50ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-2b528f98197e>:6: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(\"lifebear.csv\", sep=';', low_memory=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   id    login_id              mail_address                          password  \\\n",
            "0   1    sugimoto   sugimoto@lifebear.co.jp  f0bac04aa1b45cf443d722d6f71c0250   \n",
            "1   2         kou  nakanishi@lifebear.co.jp  48207c322ee5bb156ffec9f08c960aaa   \n",
            "2   3      yusuke     yuozawa1208@gmail.com  048261a8024ce51d379eb53cc51aaf33   \n",
            "3   4  entyan1106        endo1106@gmail.com  cd77a9dac26260a104facda5665eb3ab   \n",
            "4   5      kuriki          kuriki@wavy4.com  a026597c294cc48cd20ae361f10cbab1   \n",
            "\n",
            "            created_at          salt birthday_on  gender  \n",
            "0  2012-01-13 22:54:05  yGwBKynnsctI  1984-11-09     0.0  \n",
            "1  2012-01-14 12:48:31  aha6EuRYCDvU  1986-11-13     0.0  \n",
            "2  2012-01-17 15:33:22  PVS59dPWk9BH  1984-12-08     0.0  \n",
            "3  2012-01-17 15:37:02  vLZI6TVCJowN  1987-11-06     0.0  \n",
            "4  2012-01-17 18:52:32  swFznWWk79fg  1986-10-21     0.0  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3680442 entries, 0 to 3680441\n",
            "Data columns (total 8 columns):\n",
            " #   Column        Dtype  \n",
            "---  ------        -----  \n",
            " 0   id            int64  \n",
            " 1   login_id      object \n",
            " 2   mail_address  object \n",
            " 3   password      object \n",
            " 4   created_at    object \n",
            " 5   salt          object \n",
            " 6   birthday_on   object \n",
            " 7   gender        float64\n",
            "dtypes: float64(1), int64(1), object(6)\n",
            "memory usage: 224.6+ MB\n",
            "None\n",
            "                 id        gender\n",
            "count  3.680442e+06  2.362919e+06\n",
            "mean   4.918847e+06  6.488492e-01\n",
            "std    3.677878e+06  5.455685e-01\n",
            "min    1.000000e+00  0.000000e+00\n",
            "25%    1.339456e+06  0.000000e+00\n",
            "50%    4.577952e+06  1.000000e+00\n",
            "75%    8.188005e+06  1.000000e+00\n",
            "max    1.159593e+07  9.900000e+01\n"
          ]
        }
      ],
      "source": [
        "# prompt: read csv \"lifebear.csv\" print head, info, description and change the seperator to a ';', set to low memory = 'True'\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "df = pd.read_csv(\"lifebear.csv\", sep=';', low_memory=True)\n",
        "\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "print(df.describe())\n",
        "\n",
        "\n",
        "def split_csv_by_size(file_path, max_size_in_mb, output_prefix):\n",
        "    # Convert max size to bytes\n",
        "    max_size_in_bytes = max_size_in_mb * 1024 * 1024\n",
        "\n",
        "    # Open the input CSV file in chunks\n",
        "    chunk_iter = pd.read_csv(file_path, chunksize=1000)  # Read in 1000 rows per chunk initially\n",
        "    current_chunk = pd.DataFrame()  # Store the current chunk of data\n",
        "    current_size = 0  # Keep track of the current size in bytes\n",
        "\n",
        "    # Keep track of the part number\n",
        "    part_num = 1\n",
        "\n",
        "    # Iterate through chunks\n",
        "    for chunk in chunk_iter:\n",
        "        # Append the new chunk to the current one\n",
        "        current_chunk = pd.concat([current_chunk, chunk], ignore_index=True)\n",
        "\n",
        "        # Temporarily save the chunk and calculate the file size\n",
        "        temp_file = f\"{output_prefix}_part_{part_num}.csv\"\n",
        "        current_chunk.to_csv(temp_file, index=False)\n",
        "        current_size = os.path.getsize(temp_file)\n",
        "\n",
        "        # If the file size exceeds the max limit, save the current chunk as a final file\n",
        "        if current_size >= max_size_in_bytes:\n",
        "            print(f\"Saved {temp_file} (size: {current_size / (1024 * 1024):.2f} MB)\")\n",
        "            # Move to the next part, and reset the chunk\n",
        "            part_num += 1\n",
        "            current_chunk = pd.DataFrame()  # Reset the chunk after saving\n",
        "\n",
        "    # Save any remaining data if it's below the file size threshold\n",
        "    if not current_chunk.empty:\n",
        "        temp_file = f\"{output_prefix}_part_{part_num}.csv\"\n",
        "        current_chunk.to_csv(temp_file, index=False)\n",
        "        current_size = os.path.getsize(temp_file)\n",
        "        print(f\"Saved {temp_file} (size: {current_size / (1024 * 1024):.2f} MB)\")\n",
        "\n",
        "# Usage example:\n",
        "file_path = 'lifebear.csv'  # Path to the large CSV file\n",
        "max_size_in_mb = 500                 # Maximum size for each chunk in MB\n",
        "output_prefix = 'chunk'       # Prefix for the output files\n",
        "\n",
        "split_csv_by_size(file_path, max_size_in_mb, output_prefix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mROqRs_OUSMa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def split_csv_by_size(file_path, max_size_in_mb, output_prefix):\n",
        "    # Convert max size to bytes\n",
        "    max_size_in_bytes = max_size_in_mb * 1024 * 1024\n",
        "\n",
        "    # Open the input CSV file in chunks\n",
        "    chunk_iter = pd.read_csv(file_path, chunksize=1000)  # Read in 1000 rows per chunk initially\n",
        "    current_chunk = pd.DataFrame()  # Store the current chunk of data\n",
        "    current_size = 0  # Keep track of the current size in bytes\n",
        "\n",
        "    # Keep track of the part number\n",
        "    part_num = 1\n",
        "\n",
        "    # Iterate through chunks\n",
        "    for chunk in chunk_iter:\n",
        "        # Append the new chunk to the current one\n",
        "        current_chunk = pd.concat([current_chunk, chunk], ignore_index=True)\n",
        "\n",
        "        # Temporarily save the chunk and calculate the file size\n",
        "        temp_file = f\"{output_prefix}_part_{part_num}.csv\"\n",
        "        current_chunk.to_csv(temp_file, index=False)\n",
        "        current_size = os.path.getsize(temp_file)\n",
        "\n",
        "        # If the file size exceeds the max limit, save the current chunk as a final file\n",
        "        if current_size >= max_size_in_bytes:\n",
        "            print(f\"Saved {temp_file} (size: {current_size / (1024 * 1024):.2f} MB)\")\n",
        "            # Move to the next part, and reset the chunk\n",
        "            part_num += 1\n",
        "            current_chunk = pd.DataFrame()  # Reset the chunk after saving\n",
        "\n",
        "    # Save any remaining data if it's below the file size threshold\n",
        "    if not current_chunk.empty:\n",
        "        temp_file = f\"{output_prefix}_part_{part_num}.csv\"\n",
        "        current_chunk.to_csv(temp_file, index=False)\n",
        "        current_size = os.path.getsize(temp_file)\n",
        "        print(f\"Saved {temp_file} (size: {current_size / (1024 * 1024):.2f} MB)\")\n",
        "\n",
        "# Usage example:\n",
        "file_path = 'path_to_large_csv.csv'  # Path to the large CSV file\n",
        "max_size_in_mb = 100                 # Maximum size for each chunk in MB\n",
        "output_prefix = 'chunk'       # Prefix for the output files\n",
        "\n",
        "split_csv_by_size(file_path, max_size_in_mb, output_prefix)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKYy70EU5lrQqBxJ6VavYx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}