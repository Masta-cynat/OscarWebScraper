{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN02fZ0+HY9SV9lwDHd9uoS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Masta-cynat/OscarWebScraper/blob/main/UpdatedCleaningScript.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aqGg9DIl4AO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Read the initial CSV file with separator modification and low memory usage\n",
        "df = pd.read_csv(\"lifebear.csv\", sep=';', low_memory=True)\n",
        "\n",
        "# Display file details\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "print(df.describe())\n",
        "\n",
        "# Function to split CSV by size\n",
        "def split_csv_by_size(file_path, max_size_in_mb, output_prefix):\n",
        "    # Convert max size to bytes\n",
        "    max_size_in_bytes = max_size_in_mb * 1024 * 1024\n",
        "\n",
        "    # Open the CSV file in chunks to handle large files efficiently\n",
        "    chunk_iter = pd.read_csv(file_path, sep=';', chunksize=10000, low_memory=True)  # Read in 10,000 rows per chunk\n",
        "    current_chunk = pd.DataFrame()  # Store the current chunk of data\n",
        "    part_num = 1  # Keep track of the part number\n",
        "    current_size = 0  # Track the current file size in memory\n",
        "\n",
        "    for chunk in chunk_iter:\n",
        "        # Append the chunk to the current batch\n",
        "        current_chunk = pd.concat([current_chunk, chunk], ignore_index=True)\n",
        "\n",
        "        # Estimate the size of the current chunk in memory (rows * columns * estimated row size)\n",
        "        current_size += current_chunk.memory_usage(deep=True).sum()\n",
        "\n",
        "        # Check if current size exceeds the max limit, and save the file\n",
        "        if current_size >= max_size_in_bytes:\n",
        "            output_file = f\"{output_prefix}_part_{part_num}.csv\"\n",
        "            current_chunk.to_csv(output_file, index=False, sep=';')\n",
        "            print(f\"Saved {output_file} (size: {current_size / (1024 * 1024):.2f} MB)\")\n",
        "\n",
        "            # Reset for the next part\n",
        "            part_num += 1\n",
        "            current_chunk = pd.DataFrame()  # Clear current chunk\n",
        "            current_size = 0  # Reset size counter\n",
        "\n",
        "    # Save any remaining data that didn't reach the size threshold\n",
        "    if not current_chunk.empty:\n",
        "        output_file = f\"{output_prefix}_part_{part_num}.csv\"\n",
        "        current_chunk.to_csv(output_file, index=False, sep=';')\n",
        "        final_size = os.path.getsize(output_file)\n",
        "        print(f\"Saved {output_file} (size: {final_size / (1024 * 1024):.2f} MB)\")\n",
        "\n",
        "# Usage\n",
        "file_path = 'lifebear.csv'  # Path to the CSV file\n",
        "max_size_in_mb = 150        # Max size of each chunk in MB\n",
        "output_prefix = 'chunk'     # Output file name prefix\n",
        "\n",
        "# Split the CSV file by size\n",
        "split_csv_by_size(file_path, max_size_in_mb, output_prefix)\n"
      ]
    }
  ]
}